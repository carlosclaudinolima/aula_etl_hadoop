-----

# Laborat√≥rio de Estudos de Big Data: Stack Hadoop & Spark com Docker

## üéØ Objetivo do Projeto

Este reposit√≥rio documenta a jornada de constru√ß√£o de um ecossistema completo de Big Data, do zero, utilizando Docker Compose. O objetivo principal √© o aprendizado pr√°tico e aprofundado dos componentes fundamentais de uma arquitetura de dados moderna, desde o armazenamento distribu√≠do at√© o processamento em larga escala para ETL e Machine Learning.

O projeto foi constru√≠do de forma incremental, pe√ßa por pe√ßa, para permitir o entendimento das depend√™ncias e da intera√ß√£o entre cada servi√ßo, replicando em um ambiente local os desafios de configura√ß√£o e depura√ß√£o encontrados em sistemas de produ√ß√£o.

## üèõÔ∏è Arquitetura da Solu√ß√£o

A arquitetura implementada segue o padr√£o de um Data Lake moderno, com camadas bem definidas para ingest√£o, armazenamento, processamento e consumo de dados.

  * **Camada de Armazenamento (Data Lake):** Utiliza **HDFS** para armazenar dados em seu formato bruto (`raw`) e processado (`processed`), garantindo escalabilidade e resili√™ncia.
  * **Camada de Gerenciamento de Recursos:** O **YARN** atua como o "sistema operacional" do cluster, gerenciando os recursos de CPU e mem√≥ria para as aplica√ß√µes.
  * **Camada de Cat√°logo de Metadados:** O **Apache Hive**, atrav√©s do **Hive Metastore**, serve como um cat√°logo central e persistente para todos os dados do Data Lake, provendo uma camada de abstra√ß√£o sobre os arquivos f√≠sicos. O **Hive Server** oferece um endpoint SQL para consultas ad-hoc e integra√ß√£o com ferramentas de BI.
  * **Camada de Processamento (ETL & ML):** O **Apache Spark** √© o motor principal para o processamento de dados em larga escala. Ele √© usado para executar pipelines de ETL (lendo dados brutos, aplicando transforma√ß√µes e salvando em formatos otimizados como Parquet) e para o treinamento de modelos de Machine Learning.
  * **Camada de Orquestra√ß√£o:** O **Apache Airflow** √© utilizado para agendar, executar e monitorar os pipelines de dados de forma program√°tica.

## üõ†Ô∏è Tecnologias Utilizadas (Stack)

| Categoria | Componente | Status |
| :--- | :--- | :--- |
| **Infraestrutura** | Docker & Docker Compose | **Implementado** |
| **Armazenamento (Storage)** | HDFS (Namenode, Datanode) | **Implementado** |
| **Gerenciamento de Recursos** | YARN (ResourceManager, NodeManager) | **Implementado** |
| **Cat√°logo de Metadados** | Hive Metastore + PostgreSQL | **Implementado** |
| **Acesso SQL** | Hive Server | **Implementado** |
| **Processamento Distribu√≠do** | Apache Spark (Master, Worker) | **Implementado** |
| **Orquestra√ß√£o de Pipeline** | Apache Airflow + MySQL | Implementado (Inativo) |
| **Banco NoSQL (Serving Layer)** | Apache HBase (Master, RegionServer) | Para Implementar |
| **Coordena√ß√£o** | Apache ZooKeeper | Para Implementar |
| **Camada SQL para NoSQL** | Apache Phoenix | Para Implementar |
| **Visualiza√ß√£o de Dados (BI)**| Apache Superset ou Metabase | Para Implementar |

## üöÄ Como Executar o Projeto

1.  **Pr√©-requisitos:**

      * Docker e Docker Compose instalados.
      * Git para clonar o reposit√≥rio.

2.  **Configura√ß√£o:**

      * Clone este reposit√≥rio: `git clone <URL_DO_SEU_REPOSIT√ìRIO>`
      * Navegue para a pasta do projeto: `cd <NOME_DA_PASTA>`
      * Certifique-se de que os arquivos de configura√ß√£o `hadoop.env` e `hive-conf/hive-site.xml` est√£o presentes e corretos.

3.  **Execu√ß√£o:**

      * Para subir todo o ambiente em segundo plano, execute:
        ```bash
        docker-compose up -d
        ```
      * Para verificar o status dos servi√ßos:
        ```bash
        docker-compose ps
        ```

4.  **Acessando as Interfaces Web:**

      * **HDFS (Namenode):** `http://localhost:9870`
      * **YARN (ResourceManager):** `http://localhost:8088`
      * **Spark (Master):** `http://localhost:8081`
      * **Hive (Server2 UI):** `http://localhost:10002`

## ‚ö° Exemplos de Uso

### Executar um Job MapReduce (WordCount)

```bash
# Entrar no cont√™iner do namenode
docker exec -it namenode /bin/bash

# Criar dados de exemplo e submeter o job
hdfs dfs -mkdir -p /test
echo "hello world hello hadoop" > test.txt
hdfs dfs -put test.txt /test
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /test /output
hdfs dfs -cat /output/part-r-00000
```

### Conectar ao Hive via Beeline

```bash
# Entrar no cont√™iner do hive-server
docker exec -it hive-server /bin/bash

# Conectar ao servi√ßo
beeline -u jdbc:hive2://localhost:10000
```

### Submeter um Job PySpark

```bash
# Submeter o script 'meu_job.py' localizado na pasta 'jobs'
docker exec spark-master /spark/bin/spark-submit /opt/spark/jobs/meu_job.py
```

-----

*Este projeto √© um ambiente de estudos e n√£o se destina ao uso em produ√ß√£o.*